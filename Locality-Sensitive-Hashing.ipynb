{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project-2_Tarun.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E9KO9eLIdyoq"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5b7Ojkzdynm"
      },
      "source": [
        "# Tarun Kumar Alapati # ID A20218266"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Eqcq7tdynq"
      },
      "source": [
        "# Project-2: Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Rrwxxudynq"
      },
      "source": [
        "#Import all the necessary packages\n",
        "\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from collections import OrderedDict \n",
        "\n",
        "\n",
        "from random import randrange\n",
        "from shutil import copyfile"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KiV8xpdynt"
      },
      "source": [
        "## Execute the follwing two cells to generate your data sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnTH_wLLfDT_",
        "outputId": "67d3b6c9-a08e-4075-f4a9-ff3ed0cdf332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fX74HAAdynt"
      },
      "source": [
        "MY_ID = '266'\n",
        "# MY_ID should be string\n",
        "# Example MY_ID='819'\n",
        "n_1,n_2,n_3 ='','',''\n",
        "\n",
        "\n",
        "try:\n",
        "    n_1 = int(MY_ID[-1]) % 5\n",
        "    n_2 = int(MY_ID[-2]) % 5\n",
        "    n_3 = int(MY_ID[-3]) % 5\n",
        "    \n",
        "    while n_1 == n_2 or n_1 == n_3:\n",
        "        n_1 = (n_1 + 1) % 5\n",
        "    \n",
        "    while n_1 == n_2 or n_2 == n_3:\n",
        "        n_2 = (n_2 + 1) % 5\n",
        "        \n",
        "except Exception as e:\n",
        "    print('Please enter a valid ID...')\n",
        "\n",
        "\n",
        "task_dict = {0:'taska',1:'taskb',2:'taskc',3:'taskd',4:'taske'}\n",
        "id_dict = {0:n_1,1:n_2,2:n_3}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL-tkAYfghVe",
        "outputId": "85a0cb62-b747-493c-db65-e19b093fd959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "id_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3, 1: 1, 2: 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_RR1HGAdynz"
      },
      "source": [
        "data_path = '/content/drive/My Drive/Big Data Assignments/Project2/corpus-20090418_1' \n",
        "# Edit this path if the data directory is not in the current directory\n",
        "\n",
        "try:\n",
        "    os.makedirs('Data_Sample')\n",
        "    os.makedirs('Original_Sample')\n",
        "except Exception as e:\n",
        "    pass\n",
        "\n",
        "for i in range(3):\n",
        "    task = task_dict[id_dict[i]]\n",
        "    \n",
        "    for file in os.listdir(data_path):\n",
        "        if task in file:\n",
        "            if 'orig' in file:\n",
        "                copyfile(data_path + '/' + file, 'Original_Sample/' + file)\n",
        "            else:\n",
        "                copyfile(data_path + '/' + file, 'Data_Sample/' + file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x5C52X8dyn2"
      },
      "source": [
        "Your dataset for this project will be in <b>Data_Sample</b>\n",
        "\n",
        "\n",
        "Your query choices will be in <b>Original_Sample</b> directory\n",
        "\n",
        "\n",
        "You have to use any one original Wikipedia article from <b>Original_Sample</b> for <b>Fact Check</b> steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqhZJdDWdyn3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLNVejh-dyn6"
      },
      "source": [
        "### STEP - 1: Shingling (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-FRUik00O2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzJ_E0lD0Uc3"
      },
      "source": [
        "#Assign the value of K to the minimum shingle count \n",
        "K=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9zPOnN70Uc-"
      },
      "source": [
        "### STEP - 1: Shingling (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYYE3pdy0O21"
      },
      "source": [
        "### STEP - 1: Shingling (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7dzLWL1dyn7"
      },
      "source": [
        "# Type your code here... \n",
        "# Create necessary number of cells below this cell\n",
        "\n",
        "#Function returns all the three shingle, four shingle and Five Shingles at once for a file\n",
        "def get_shingle(size,f):\n",
        "    #shingles = set()\n",
        "    three_shingles=[]\n",
        "    four_shingles=[]\n",
        "    five_shingles=[]\n",
        "    #print(f)\n",
        "    for i in range (0,len(f)-2+1):\n",
        "         three_shingles.append(tuple(f[i:i+size]))\n",
        "         four_shingles.append(tuple(f[i:i+size+1]))\n",
        "         five_shingles.append(tuple(f[i:i+size+2]))\n",
        "    return three_shingles,four_shingles,five_shingles\n",
        "\n",
        "#Function reads files that are called converts the text in to lower and returns all three four and five shingles\n",
        "\n",
        "def get_shingles_file(file, k):\n",
        "  with open(('/content/Data_Sample/'+file),encoding ='utf8',errors='ignore') as f:\n",
        "    matrix=[words for words in f.read().lower().split()]\n",
        "    final_shingles=[]\n",
        "    #for line in matrix: \n",
        "    three_shingles,four_shingles,five_shingles =  get_shingle(k, matrix)\n",
        "    f.close()\n",
        "  return three_shingles, four_shingles, five_shingles\n",
        "\n",
        "\n",
        "# We iterate through all the files in Data Sample folder and get unique five shingles from all the documents. Got all the five shingles and respective files in a dictionary\n",
        "\n",
        "glob_shingles=[]\n",
        "five_list_shingles=[]\n",
        "dict_shingles={}\n",
        "five_dict_shingles={}\n",
        "\n",
        "for file in os.listdir('/content/Data_Sample'):\n",
        "    #print(get_shingles_file(file, 3))\n",
        "    glob_shingles=glob_shingles+get_shingles_file(file, 3)[0]+get_shingles_file(file, 3)[1]+get_shingles_file(file, 3)[2]\n",
        "    five_list_shingles=five_list_shingles+get_shingles_file(file, 3)[2]\n",
        "    dict_shingles[file]=get_shingles_file(file, 3)[0]+get_shingles_file(file, 3)[1]+get_shingles_file(file, 3)[2]\n",
        "    five_dict_shingles[file]=list(set(get_shingles_file(file, 3)[2]))\n",
        "\n",
        "\n",
        "glob_shingles = list(set(glob_shingles))\n",
        "five_list_shingles_unique = [v for v in list(set(five_list_shingles)) if len(v)==5 ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgivovaMIpo",
        "outputId": "f5a77506-13f9-4949-91d4-d9abce358c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(five_list_shingles))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHN33EOXFSNr",
        "outputId": "5748edec-7338-4b86-eabc-6ee387721da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(five_list_shingles_unique)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQM-fLWU-dv3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlfsM-i116Qw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov9JzptkdyoC"
      },
      "source": [
        "Report results (number of unique k-shingles) for k={3,4,5} below:\n",
        "1. k=3:\n",
        "2. k=4:\n",
        "3. k=5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFsywycn_45J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_OGL_ddyoC"
      },
      "source": [
        "# Return all the unique three, four and Five shingles counts from all the documents\n",
        "count_5=0\n",
        "count_4=0\n",
        "count_3=0\n",
        "for i, value in enumerate(glob_shingles):\n",
        "  if len(value)== 5:\n",
        "    count_5=count_5+1\n",
        "  elif len(value)== 4:\n",
        "    count_4=count_4+1\n",
        "  elif len(value)== 3:\n",
        "    count_3=count_3+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi0xhUaXdyoE",
        "outputId": "2a33a577-a0cc-4115-ed04-570e2d97bc72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(count_3)\n",
        "print(count_4)\n",
        "print(count_5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7929\n",
            "8721\n",
            "9120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuWkBxu4dyoG"
      },
      "source": [
        "### STEP - 2: Min-Hashing (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDBjuDTydyoH"
      },
      "source": [
        "# ***************************************************NEW***********************************************************\n",
        "# Generate Hash functions - \n",
        "    # We use (ax + b) mod N formula to permute shingle index\n",
        "    # where a,b are random numbers, N total index size, and x is the index\n",
        "# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes\n",
        "# Following function takes total index size N and L as arguments\n",
        "    # And returns L new lists of size N\n",
        "    \n",
        "def get_hash_functions(N,L):\n",
        "    hash_functions = []\n",
        "    \n",
        "    for itr in range(L):\n",
        "        a=randrange(1,400)\n",
        "        b=randrange(1,400)\n",
        "        \n",
        "        new_hash_function = []\n",
        "        for i in range(N):\n",
        "            new_hash_function.append((a * i + b) % N)\n",
        "        \n",
        "        hash_functions.append(new_hash_function)\n",
        "    return hash_functions\n",
        "        \n",
        "# test\n",
        "# hash_functions = get_hash_functions(5000,50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKsLsj50dyoJ"
      },
      "source": [
        "# Type your code here to generate all L hash functions\n",
        "# Generate hash functions only for shingle index created for k=5\n",
        "L50 = get_hash_functions(count_5,50)\n",
        "L100 = get_hash_functions(count_5,100)\n",
        "L200 = get_hash_functions(count_5,200)\n",
        "L500 = get_hash_functions(count_5,500)\n",
        "L1000 = get_hash_functions(count_5,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPFL4-PCdyoM"
      },
      "source": [
        "# Generates Signatire Matrix and return signature matrix and Dictionary that has signature values as arrays and files as their respective keys\n",
        "\n",
        "\n",
        "def genSigMatrix(five_dict_shingles, five_list_shingles_unique, hashValues):\n",
        "  sig = []\n",
        "  sig_file_dict={}\n",
        "  for file in five_dict_shingles:\n",
        "    list_shingle = [math.inf for i in range(len(hashValues))]\n",
        "    for j,i in enumerate(five_list_shingles_unique):\n",
        "      if i in five_dict_shingles[file]:\n",
        "        for k in range(len(hashValues)):\n",
        "          if hashValues[k][j] < list_shingle[k]:\n",
        "             list_shingle[k] = hashValues[k][j]\n",
        "    sig.append(list_shingle)\n",
        "    sig_file_dict[file]=list_shingle\n",
        "  sigmatrix = np.matrix(sig).T\n",
        "  return sigmatrix,sig_file_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opaqyA_bdyoO"
      },
      "source": [
        "#Get Signature Matrix for the respective Hash Functions \n",
        "\n",
        "Sig50= genSigMatrix(five_dict_shingles,five_list_shingles_unique, L50)[0]\n",
        "Sig100= genSigMatrix(five_dict_shingles,five_list_shingles_unique, L100)[0]\n",
        "Sig200= genSigMatrix(five_dict_shingles,five_list_shingles_unique, L200)[0]\n",
        "Sig500= genSigMatrix(five_dict_shingles,five_list_shingles_unique, L500)[0]\n",
        "Sig1000= genSigMatrix(five_dict_shingles,five_list_shingles_unique, L1000)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXcySVg82joS",
        "outputId": "602d2826-86e0-40c2-ee21-b893ccfb729a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Getting Signature Matrix index\n",
        "Sig1000.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 57)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75VIqkl5jZ9e"
      },
      "source": [
        "#A dictionary that has signature values as arrays and files as their respective keys\n",
        "sig_file_dict=genSigMatrix(five_dict_shingles,five_list_shingles_unique, L1000)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdG7SV56dyoR"
      },
      "source": [
        "# Type your code here to do the fact check \n",
        "#      with any one query document in the 'Original_Sample' directory\n",
        "#This returns Jac_Dict that has similar files that has jacardian similarity with greater than0.2\n",
        "\n",
        "original = 'Original_Sample/orig_taskc.txt'\n",
        "\n",
        "# STEP-1: Generate 5-shingles \n",
        "    # (if any shingles are not present in your shingle index, simply ignore them)\n",
        "orig_five_shingles = get_shingles_file(file, K)[2]\n",
        "orig_five_shingles_unique = [v for v in list(set(five_list_shingles)) if len(v)==5 ]\n",
        "   \n",
        "    \n",
        "# STEP-2: Generate signature vector from L hash functions\n",
        "def genSigVectorOrig(orig_five_shingles_unique, five_list_shingles_unique, hashValues):\n",
        "  list_shingle = [math.inf for i in range(len(hashValues))]\n",
        "  for j,i in enumerate(five_list_shingles_unique):\n",
        "      if i in orig_five_shingles_unique:\n",
        "        for k in range(len(hashValues)):\n",
        "          if hashValues[k][j] < list_shingle[k]:\n",
        "             list_shingle[k] = hashValues[k][j]\n",
        "  sigmatrix = np.matrix(list_shingle).T\n",
        "  return sigmatrix\n",
        "\n",
        "sig_org=genSigVectorOrig(orig_five_shingles_unique, five_list_shingles_unique, L1000)\n",
        "\n",
        "# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n",
        "    # and all other documents \n",
        "def getJacSim(sig_file_dict, sig_org, hashValues, t=0.1):\n",
        "  jaccard = {}\n",
        "  for file in sig_file_dict:\n",
        "    count =0\n",
        "    for j in range(len(sig_file_dict[file])):\n",
        "      if sig_file_dict[file][j] == sig_org[j,0]:\n",
        "        count = count+1\n",
        "    jaccardValue = count/len(sig_file_dict[file])\n",
        "    if jaccardValue>t:\n",
        "      jaccard[file]=jaccardValue\n",
        "  return jaccard\n",
        "\n",
        "jac_dict = getJacSim(sig_file_dict, sig_org, L1000, t=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwz-3xUglYDs"
      },
      "source": [
        "jac_dict_sort = sorted(jac_dict.items(),key = lambda x : x[1], reverse = True)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVFRTJ5SGKYt"
      },
      "source": [
        "#Sort Dictionary  Sin decreasing order of the similary\n",
        "jac_dict_sort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_safBGdyoT"
      },
      "source": [
        "***************************************************NEW***********************************************************\n",
        "For each L = {50,100,200,500,1000}, report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
        "Sort the documents in decreasing order of the Jaccard similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA5Vlj0KdyoU",
        "outputId": "eb74b91a-ba7c-4e46-e581-1319f375586c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufcUtH3YdyoW"
      },
      "source": [
        "### STEP - 3: LSH (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fwSc4Z7dyoZ"
      },
      "source": [
        "doc_list = os.listdir('/content/Data_Sample')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rotBzoN6w64a"
      },
      "source": [
        "doc_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5piWFgehdyoW"
      },
      "source": [
        "# Type your code here to hash signature matrix into B buckets\n",
        "# Use the technique to split the signature matrix into b bands of r rows\n",
        "# Convert only the signature matrix generated with L=1000\n",
        "\n",
        "#Generate Hash Buckets using reshape with bands\n",
        "\n",
        "b = 50\n",
        "r = 20\n",
        "B = 199\n",
        "reshape1000 = np.array(Sig1000)\n",
        "reshape1000= reshape1000.reshape((b,r,len(doc_list)))\n",
        "a=random.sample(range(100), r)\n",
        "\n",
        "docsBucket = {}\n",
        "for i in range(b):\n",
        "  dict1 = collections.defaultdict(list)\n",
        "  for j in range(len(doc_list)):\n",
        "    hashfunction = (np.sum((a)*(reshape1000[i,:,j])))%B#hash signature\n",
        "    dict1[hashfunction].append(doc_list[j])\n",
        "  docsBucket.update(dict1)\n",
        "\n",
        "finalBucket= collections.defaultdict(set) # storing the candidate items\n",
        "for item in docsBucket: \n",
        "  if len(docsBucket[item]) > 1: #considering the bucket only if it has candidate item\n",
        "    for i in docsBucket[item]:\n",
        "      finalBucket[item].add(i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax_4o6g50we2",
        "outputId": "3803d286-c1eb-46a4-f64d-4d5f41bf4050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# STEP - 1: Split your original document signature vector into b bands of r rows\n",
        "vec_org = np.array(sig_org)\n",
        "vec_org_reshaped = vec_org.reshape((b,r,1))\n",
        "# STEP - 2: Hash using the same hash functions created for \n",
        "    # signature matrix hashing (in the previous cell)\n",
        "vectorBucket = collections.defaultdict(list)\n",
        "for i in range(b):\n",
        "  for j in range(1):\n",
        "    hashfunction = (np.sum((a)*(vec_org_reshaped[i,:,j])))%B\n",
        "    if finalBucket[hashfunction] == set():\n",
        "      continue\n",
        "    else:\n",
        "      vectorBucket[hashfunction].append(finalBucket[hashfunction])\n",
        "\n",
        "vectorCandidates = set() #Define Vector Candidate as a set\n",
        "for i in vectorBucket: \n",
        "  for j in vectorBucket[i]: # Iterate each key for its value\n",
        "    for item in j:\n",
        "      vectorCandidates.add(item) # Storing all the documents into candidate items\n",
        "vectorCandidates = list(vectorCandidates)\n",
        "print(len(vectorCandidates))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlTNl7jTdyob"
      },
      "source": [
        "candidatesDict = {}\n",
        "for i in vectorCandidates: \n",
        "  #print(i)\n",
        "  for j in five_dict_shingles:\n",
        "    if i == j:\n",
        "      candidatesDict[i] = five_dict_shingles[j]\n",
        "#print(candidatesDict) # This will form all the candidate_items with their content stored in it\n",
        "candidatesDictItems = [] # This stores all the 5-shingles from each candidate_item into one list\n",
        "for key,value in candidatesDict.items(): # Below is the loop running for obtaining them\n",
        "  for item in value:\n",
        "    candidatesDictItems.append(item)\n",
        "print(candidatesDictItems)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gv8UE_fdyod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmo1SSoidyof",
        "outputId": "a5e904f3-e2f2-4393-b916-f4559ead2ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Type your code here to do the fact check\n",
        "# Calculate Jaccard similarity of the roiginal document with only \n",
        "    # candidate documents\n",
        "\n",
        "### Get Signature Matrix and Dict using Candidate Items\n",
        "candidateSignatureMatrix = genSigMatrix(candidatesDict, candidatesDictItems, L1000)[0]\n",
        "\n",
        "candidateSignature_dict=genSigMatrix(candidatesDict, candidatesDictItems, L1000)[1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF_C5K39dyoj"
      },
      "source": [
        "similar_doc= getJacSim(candidateSignature_dict, sig_org, L1000, t=0.2)\n",
        "similar_doc_sorted= sorted(similar_doc.items(),key = lambda x : x[1], reverse = True)"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UWQmo6Idyol"
      },
      "source": [
        "Report all documents (file_names) below that have Jaccard similarity > t=0.85\n",
        "Sort the documents in decreasing order of the Jaccard similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIZ9r8Tdyol",
        "outputId": "5b3d63ed-4697-49be-fc98-34dea10903f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print Candidate Documents that are with Jaccardian Similarity >0.2\n",
        "print(similar_doc_sorted)\n"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgUEpkEMdyon"
      },
      "source": [
        "Report the list of false positives and false negatives below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYD8E7Sdyon",
        "outputId": "6cdbfcb9-24cf-478e-e7b8-19fba262346b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "total_doc = candidatesDict.keys() # Considers to check if the candidate documents keys are \n",
        "similar_docs = similar_doc.keys() # Documents that are similar in sorted fashion are obtained\n",
        "false_pos = total_doc - similar_docs # Stores the set of all \n",
        "print('Count of false positives = '+str(len(false_pos)))\n",
        "print('False Postives are ' + str(false_pos))\n",
        "original_doc = jac_dict.keys()\n",
        "false_negatives = original_doc-similar_docs\n",
        "print('Count of false positives = '+str(len(false_negatives)))\n",
        "print('False Postives are ' + str(false_negatives))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of false positives = 1\n",
            "False Postives are {'g2pC_taskb.txt'}\n",
            "Count of false positives = 12\n",
            "False Postives are {'g0pA_taskc.txt', 'g4pB_taskd.txt', 'g1pD_taskb.txt', 'g3pB_taskb.txt', 'g0pB_taskb.txt', 'g3pA_taskc.txt', 'g2pA_taskb.txt', 'g1pA_taskb.txt', 'g4pC_taskd.txt', 'g3pB_taskc.txt', 'g0pB_taskc.txt', 'g3pA_taskd.txt'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9KO9eLIdyoq"
      },
      "source": [
        "### BONUS : 10 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl52_WyNdyot"
      },
      "source": [
        "# Experiment with different values of b,r,B, and t\n",
        "    # to reduce the number of false positives and false negatives\n",
        "    # Report all results in a table in a separate word document"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}